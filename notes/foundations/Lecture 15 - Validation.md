# Lecture 15 - Validation

上节课我们主要讲了为了避免 overfitting，可以使用 regularization 方法来解决。在之前的 ${E_{in}}$ 上加上一个 regularizer，生成 ${E_{aug}}$，将其最小化，这样可以有效减少模型的复杂度，避免过拟合现象的发生。那么，机器学习领域还有许多选择，如何保证训练的模型具有良好的泛化能力？本节课将介绍一些概念和方法来解决这个选择性的问题。

## Model Selection Problem

机器学习模型建立的过程中有许多选择，例如对于简单的二元分类问题，首先是算法 ${A}$ 的选择，有 PLA，pocket，linear regression，logistic regression 等等；其次是迭代次数 ${T}$ 的选择，有 ${100}$，${1000}$,${10000}$ 等等；之后是学习速率 ${\eta}$ 的选择，有 ${1}$，${0.01}$, ${0.0001}$ 等等；接着是模型特征转换 ${\Phi}$ 的选择，有 linear，quadratic，poly-10，Legendre-poly-10 等等；然后是正则化 regularizer的选择，有 ${L1}$，${L2}$ 等等；最后是正则化系数 ${\lambda}$ 的选择，有 ${0}$， ${0.01}$，${1}$ 等等。不同的选择搭配，有不同的机器学习效果。我们的目标就是找到最合适的选择搭配，得到一个好的 ${g}$，构建最佳的机器学习模型。

![So Many Models Learned](http://ofqm89vhw.bkt.clouddn.com/0463627872d6eea4e14add9d53e05c13.png)

假设有 ${M}$ 个模型，对应有 ${H_1,\ H_2, \cdots ,H_M}$，即有 ${M}$ 个 hypothesis set，演算法为 ${A_1, A_2,\cdots ,A_M}$，共 ${M}$ 个。我们的目标是从这 ${M}$ 个 hypothesis set 中选择一个模型 ${H_{m\ast}}$，通过演算法 ${A_{m\ast}}$ 对样本集 ${D}$ 的训练，得到一个最好的矩 ${g_{m\ast}}$，使其 ${E_{out}(g_{m\ast})}$ 最小。所以，问题的关键就是机器学习中如何选择到最好的矩${g_{m\ast}}$。

考虑有这样一种方法，对 ${M}$ 个模型分别计算使 ${E_{in}}$ 最小的 ${g}$，再横向比较，取其中能使 ${E_{in}}$ 最小的模型的矩${g_{m\ast}}$：

![Model Selection](http://ofqm89vhw.bkt.clouddn.com/8d590e010cc91235a3a8cf0370853183.png)

但是 ${E_{in}}$ 足够小并不能表示模型好，反而可能表示训练的矩 ${g_{m\ast}}$ 发生了过拟合，泛化能力很差。而且这种“模型选择+学习训练”的过程，它的VC Dimension是 ${d_{VC}(H_1 \cup H_2)}$，模型复杂度增加。总的来说，泛化能力差，用 ${E_{in}}$ 来选择模型是不好的。

另外一种方法，如果有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下 ${E_{test}}$ 的大小，则选取 ${E_{test}}$ 最小的模型作为最佳模型。

这种测试集验证的方法，根据 finite-bin Hoffding 不等式，可以得到。

$${E_{out}(g_{m\ast}) \leq E_{test}(g_{m\ast})+O(\sqrt{ \frac{log^M}{N_{test}}})}$$

由上式可以看出，模型个数 ${M}$ 越少，测试集数目越大，那么 ${O(\sqrt{ \frac{log^M}{N_{test}}})}$ 越小，即 ${E_{test} (g_{m\ast})}$ 越接近于 ${E_{out} (g_{m\ast})}$。

![Comparison](http://ofqm89vhw.bkt.clouddn.com/daced18e57ca86cf8ad1130b01705b61.png)

比较一下之前讲的两种方法，第一种方法使用 ${E_{in}}$ 作为判断基准，使用的数据集就是训练集 ${D}$ 本身；第二种方法使用 ${E_{test}}$ 作为判断基准，使用的是独立于训练集 ${D}$ 之外的测试集。前者不仅使用 ${D}$ 来训练不同的 ${g_m}$，而且又使用 ${D}$ 来选择最好的${g_{m\ast}}$，那么${g_{m\ast}}$对未知数据并不一定泛化能力好。举个例子，这相当于老师用学生做过的练习题再来对学生进行考试，那么即使学生得到高分，也不能说明他的学习能力强。所以最小化 ${E_{in}}$ 的方法并不科学。而后者使用的是独立于 ${D}$ 的测试集，相当于新的考试题能更好地反映学生的真实水平，所以最小化 ${E_{test}}$ 更加理想。

但是，我们拿到的一都是训练集 ${D}$，测试集是拿不到的。所以，寻找一种折中的办法，我们可以使用已有的训练集 ${D}$ 来创造一个验证集 ${validation\ set}$，即从 ${D}$ 中划出一部分 ${D_{val}}$ 作为验证集。${D}$ 另外的部分作为训练模型使用，${D_{val}}$ 独立开来，用来测试各个模型的好坏，最小化 ${E_{val}}$，从而选择最佳的 ${g_{m\ast}}$。

## Validation

从训练集 ${D}$ 中抽出一部分 ${K}$ 个数据作为验证集 ${D_{val}}$，${D_{val}}$ 对应的error记为 ${E_{val}}$ 。这样做的一个前提是保证 ${D_{val}}$ 独立同分布（${iid}$）于 ${P(x,y)}$，也就是说 ${D_{val}}$ 的选择是从 ${D}$ 中平均随机抽样得到的，这样能够把 ${E_{val}}$ 与 ${E_{out}}$ 联系起来。${D}$ 中去除 ${D_{val}}$ 后的数据就是供模型选择的训练数据 ${D_{train}}$，其大小为 ${N-k}$。从 ${D_{train}}$ 中选择最好的矩，记为 ${g^{-}(m)}$ 。

![Validation Set](http://ofqm89vhw.bkt.clouddn.com/d5e1aae2e49f7e3a951c9b365fcd959e.png)

假如 ${D}$ 共有 ${1000}$ 个样本，那么可以选择其中900个 ${D_{train}}$，剩下的 ${100}$ 个作为 ${D_{val}}$ 。使用 ${D_{train}}$ 训练模型，得到最佳的 ${g^{-}(m)}$，使用 ${g^{-}(m)}$ 对 ${D_{val}}$ 进行验证，得到如下 Hoffding 不等式：

$${E_{out}(g^{-}) \leq E_{test}(g^{-})+O(\sqrt{ \frac{log^M}{K}})}$$

假设有 ${M}$ 种模型 ${hypothesis\ set}$，${D_{val}}$ 的数量为 ${K}$，那么从每种模型 ${m}$ 中得到一个在 ${D_{val}}$ 上表现最好的矩，再横向比较，从 ${M}$ 个矩中选择一个最好的 ${m_{\ast}}$ 作为我们最终得到的模型。

现在由于数量为 ${N}$ 的总样本 ${D}$ 的一部分 ${K}$ 作为验证集，那么只有 ${N-k}$ 个样本可供训练。从 ${D_{train}}$中得到最好的 ${g^{-}(m\ast)}$，而总样本 ${D}$ 对应的最好的矩为${g_{m\ast}}$。根据之前的 leraning curve 很容易知道，训练样本越多，得到的模型越准确，其 ${hypothesis}$ 越接近 ${target\ function}$，即 ${D}$ 的 ${E_{out}}$ 比 ${D_{train}}$ 的 ${E_{out}}$ 要小：

所以，我们通常的做法是通过 ${D_{val}}$ 来选择最好的矩 ${g^{-}(m\ast)}$ 对应的模型 ${m\ast}$，再对整体样本集 ${D}$ 使用该模型进行训练，最终得到最好的矩${g_{m\ast}}$。

总结一下，使用验证集进行模型选择的整个过程为：先将 ${D}$ 分成两个部分，一个是训练样本 ${D_{train}}$，一个是验证集 ${D_{val}}$ 。若有 ${M}$ 个模型，那么分别对每个模型在 ${D_{train}}$ 上进行训练，得到矩 ${g^{-}(m)}$，再用 ${D_{val}}$ 对每个 ${g^{-}(m)}$ 进行验证，选择表现最好的矩 ${g^{-}(m)}$ ∗，则该矩对应的模型被选择。最后使用该模型对整个 ${D}$ 进行训练，得到最终的${g_{m\ast}}$。下图展示了整个模型选择的过程：

![Model Selection](http://ofqm89vhw.bkt.clouddn.com/52853a1aeca6f0aa61eb28c89f48efeb.png)

不等式关系满足：

$${E_{out}(g_{m\ast}) \leq E_{out}(g_{m\ast}^{-}) \leq E_{val}(g_{m\ast}^{-})+O(\sqrt{ \frac{log^M}{K}})}$$

下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是 ${5}$ 阶多项式 ${H_{\Phi_5}}$，一个是 ${10}$ 阶多项式 ${H_{\Phi_{10}}}$。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如下：

![Validation in Practice](http://ofqm89vhw.bkt.clouddn.com/e247f7a7a62b4874b893c4ebe901c6b7.png)

图中，横坐标表示验证集数量 ${K}$，纵坐标表示 ${E_{out}}$ 大小。黑色水平线表示没有验证集，完全使用 ${E_{in}}$ 进行判断基准，那么 ${H_{\Phi_{10}}}$ 更好一些，但是这种方法的 ${E_{out}}$ 比较大，而且与 ${K}$ 无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 ${E_{out}}$ 很小，同样也与 ${K}$ 无关，实际中很难得到这条虚线。红色曲线表示使用验证集，但是最终选取的矩是 ${g^{-}(m\ast)}$，其趋势是随着 ${K}$ 的增加，它对应的 ${E_{out}}$ 先减小再增大，当 ${K}$ 大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是 ${g_{m\ast}}$，其趋势是随着 ${K}$ 的增加，它对应的 ${E_{out}}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。

这里提一点，当 ${K}$ 大于一定的值时，红色曲线会超过黑色直线。这是因为随着 ${K}$ 的增大，${D_{val}}$ 增大，但可供模型训练的 ${D_{train}}$ 在减小，那得到的 ${g^{-}(m\ast)}$ 不具有很好的泛化能力，即对应的 ${E_{out}}$ 会增大，甚至当 ${K}$ 增大到一定值时，比 ${E_{in}}$ 模型更差。

那么，如何设置验证集 ${K}$ 值的大小呢？根据之前的分析：

![The Dilemma about K](http://ofqm89vhw.bkt.clouddn.com/2f8746bb5ecb0412f6ba8346bed8a157.png)

当 ${K}$ 值很大时，${E_{val} \approx E_{out}}$，但是 ${g^{-}(m)}$ 与 ${g(m)}$ 相差很大；当 ${K}$ 值很小是，${g^{-}(m) \approx g(m)}$，但是 ${E_{val}}$ 与 ${E_{out}}$ 可能相差很大。所以有个折中的办法，通常设置 ${k = \frac{N}{5}}$。值得一提的是，划分验证集，通常并不会增加整体时间复杂度，反而会减少，因为 ${D_{train}}$ 减少了。

## Leave-One-Out Cross Validation

假如考虑一个极端的例子，${k=1}$，也就是说验证集大小为 ${1}$，即每次只用一组数据对 ${g(m)}$ 进行验证。这样做的优点是 ${g^{-}(m) \approx g(m)}$，但是 ${E_{val}}$ 与 ${E_{out}}$ 可能相差很大。为了避免 ${E_{val}}$ 与 ${E_{out}}$ 相差很大，每次从 ${D}$ 中取一组作为验证集，直到所有样本都作过验证集，共计算 ${N}$ 次，最后对验证误差求平均，得到 ${E_{loocv}(H,A)}$，这种方法称之为留一法交叉验证，表达式为：

$${E_{loocv}(H,A) = \frac{1}{N} \sum_{n=1}^{N} e_n = \frac{1}{N} \sum_{n=1}^{N} err(g_n^-(x_n),y_n)}$$

这样求平均的目的是为了让 ${E_{loocv}(H,A)}$ 尽可能地接近 ${E_{out}(g)}$ 。

下面用一个例子图解留一法的过程：

![Illustration of Leave-One-Out](http://ofqm89vhw.bkt.clouddn.com/34dcac0b5c4bc92a36b72aa2c6aebeb5.png)

如上图所示，要对二维平面上的三个点做拟合，上面三个图表示的是线性模型，下面三个图表示的是常数模型。对于两种模型，分别使用留一交叉验证法来计算 ${E_{loocv}}$，计算过程都是每次将一个点作为验证集，其他两个点作为训练集，最终将得到的验证误差求平均值，就得到了 ${E_{loocv}(linear)}$ 和 ${E_{loocv}(constant)}$，比较两个值的大小，取值小对应的模型即为最佳模型。

接下来，我们从理论上分析 Leave-One-Out 方法的可行性，即 ${E_{loocv}(H,A)}$ 是否能保证 ${E_{out}}$ 的矩足够好？假设有不同的数据集 ${D}$，它的期望分布记为 ${\epsilon_D}$，则其${E_{loocv}(H,A)}$ 可以通过推导，等于 ${E_{out}(N-1)}$ 的平均值。由于 ${N-1}$ 近似为 ${N}$，${E_{out}(N-1)}$ 的平均值也近似等于 ${E_{out}(N)}$ 的平均值。具体推导过程如下：

![Theoretical Guarantee of Leave-One-Out Estimate](http://ofqm89vhw.bkt.clouddn.com/1bc447ba7a904cd47aa0fe489627b01e.png)

最终我们得到的结论是 ${E_{loocv}(H,A)}$ 的期望值和 ${E_{out}(g-)}$ 的期望值是相近的，这代表得到了比较理想的${E_{out}(g)}$，Leave-One-Out 方法是可行的。

举一个例子，使用两个特征：Average Intensity 和 Symmetry 加上这两个特征的非线性变换（例如高阶项）来进行手写数字识别。

![Leave-One-OutinPreprlacaementcstice](http://ofqm89vhw.bkt.clouddn.com/de7939e2e0737f964fb23542e89d5005.png)

从图中我们看出，随着特征数量的增加，${E_{in}}$ 不断减小，${E_{out}}$ 先减小再增大，虽然 ${E_{in}}$ 是不断减小的，但是它与 ${E_{out}}$ 的差距越来越大，发生了过拟合，泛化能力太差。而 ${E_{cv}}$ 与 ${E_{out}}$ 的分布基本一致，能较好地反映 ${E_{out}}$ 的变化。所以，我们只要使用 Leave-One-Out 方法得到使 ${E_{cv}}$ 最小的模型，就能保证其 ${E_{out}}$ 足够小。下图是分别使用 ${E_{in}}$ 和 ${E_{out}}$ 进行训练得到的分类曲线。

很明显可以看出，使用 ${E_{in}}$ 发生了过拟合，而 ${E_{loocv}}$ 分类效果更好，泛化能力强。

## V-Fold Cross Validation

接下来我们看看 Leave-One-Out 可能的问题是什么。首先，第一个问题是计算量，假设 ${N=1000}$，那么就需要计算 ${1000}$ 次的 ${E_{loocv}}$，再计算其平均值。当 ${N}$ 很大的时候，计算量是巨大的，很耗费时间。第二个问题是稳定性，例如对于二分类问题，取值只有 ${0}$ 和 ${1}$ 两种，预测本身存在不稳定的因素，那么对所有的 ${E_{loocv}}$ 计算平均值可能会带来很大的数值跳动，稳定性不好。所以，这两个因素决定了 Leave-One-Out 方法在实际中并不常用。

针对 Leave-One-Out 的缺点，我们对其作出了改进。 Leave-One-Out 是将 ${N}$ 个数据分成 ${N}$ 分，那么改进措施是将 ${N}$ 个数据分成 ${V}$ 份（例如 ${V=10}$），计算过程与 Leave-One-Out 相似。这样可以减少总的计算量，又能进行交叉验证，得到最好的矩，这种方法称为 ${V}$-折交叉验证。其实 Leave-One-Out 就是 ${V}$-折交叉验证的一个极端例子。

$${E_{cv}(H,A) = \frac{1}{V}\sum_{v=1}^{V} E_{val}^V(g_V^-)}$$

所以呢，一般的 Validation 使用 ${V}$-折交叉验证来选择最佳的模型。值得一提的是 Validation 的数据来源也是样本集中的，所以并不能保证交叉验证的效果好，它的模型一定好。只有样本数据越多，越广泛，那么 Validation 的结果越可信，其选择的模型泛化能力越强。

## 总结

本节课主要介绍了 Validation 验证。先从如何选择一个好的模型开始切入，例如使用 ${E_{in}}$、${E_{test}}$ 都是不太好的，最终使用 ${E_{val}}$ 来进行模型选择。然后详细介绍了 Validation 的过程。最后，介绍了 Leave-One-Out 和 ${V}$-Fold Cross 两种验证方法，比较它们各自的优点和缺点，实际情况下，${V}$-Fold Cross 更加常用。

## 参考

1. [台湾大学林轩田机器学习基石课程学习笔记15 -- Validation](http://blog.csdn.net/red_stone1/article/details/72834968)