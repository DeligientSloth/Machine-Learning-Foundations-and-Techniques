# note15

上节课我们主要讲了为了避免overfitting，可以使用regularization方法来解决。在之前的 ${E_{in}}$ 上加上一个regularizer，生成 ${E_{aug}}$，将其最小化，这样可以有效减少模型的复杂度，避免过拟合现象的发生。那么，机器学习领域还有许多选择，如何保证训练的模型具有良好的泛化能力？本节课将介绍一些概念和方法来解决这个选择性的问题。

## Model Selection Problem

机器学习模型建立的过程中有许多选择，例如对于简单的二元分类问题，首先是算法A的选择，有PLA，pocket，linear regression，logistic regression等等；其次是迭代次数 ${T}$ 的选择，有100，1000,10000等等；之后是学习速率η的选择，有1，0.01,0.0001等等；接着是模型特征转换Φ的选择，有linear，quadratic，poly-10，Legendre-poly-10等等；然后是正则化regularizer的选择，有L2，L1等等；最后是正则化系数λ的选择，有0，0.01，1等等。不同的选择搭配，有不同的机器学习效果。我们的目标就是找到最合适的选择搭配，得到一个好的矩 ${g}$，构建最佳的机器学习模型。

假设有M个模型，对应有 ${H_1,H_2, \cdots ,HM}$，即有 ${M}$ 个hypothesis set，演算法为 ${A_1, A_2,\cdots ,AM}$，共 ${M}$ 个。我们的目标是从这 ${M}$ 个hypothesis set中选择一个模型Hm∗，通过演算法Am∗对样本集D的训练，得到一个最好的矩gm∗，使其 ${E_{out}(gm∗)}$ 最小。所以，问题的关键就是机器学习中如何选择到最好的矩gm∗。

考虑有这样一种方法，对M个模型分别计算使 ${E_{in}}$ 最小的矩g，再横向比较，取其中能使 ${E_{in}}$ 最小的模型的矩gm∗：

但是 ${E_{in}}$ 足够小并不能表示模型好，反而可能表示训练的矩gm∗发生了过拟合，泛化能力很差。而且这种“模型选择+学习训练”的过程，它的VC Dimension是dVC(H1∪H2)，模型复杂度增加。总的来说，泛化能力差，用 ${E_{in}}$ 来选择模型是不好的。

另外一种方法，如果有这样一个独立于训练样本的测试集，将M个模型在测试集上进行测试，看一下 ${E_{test}}$ 的大小，则选取 ${E_{test}}$ 最小的模型作为最佳模型：

这种测试集验证的方法，根据finite-bin Hoffding不等式，可以得到：

由上式可以看出，模型个数M越少，测试集数目越大，那么O(logMNtest−−−−√)越小，即 ${E_{test} (gm∗)}$ 越接近于 ${E_{out} (gm∗)}$。

下面比较一下之前讲的两种方法，第一种方法使用 ${E_{in}}$ 作为判断基准，使用的数据集就是训练集D本身；第二种方法使用 ${E_{test}}$ 作为判断基准，使用的是独立于训练集D之外的测试集。前者不仅使用 ${D}$ 来训练不同的gm，而且又使用 ${D}$ 来选择最好的gm∗，那么gm∗对未知数据并不一定泛化能力好。举个例子，这相当于老师用学生做过的练习题再来对学生进行考试，那么即使学生得到高分，也不能说明他的学习能力强。所以最小化 ${E_{in}}$ 的方法并不科学。而后者使用的是独立于D的测试集，相当于新的考试题能更好地反映学生的真实水平，所以最小化 ${E_{test}}$ 更加理想。

但是，我们拿到的一都是训练集 ${D}$，测试集是拿不到的。所以，寻找一种折中的办法，我们可以使用已有的训练集 ${D}$ 来创造一个验证集 ${validation\ set}$，即从 ${D}$ 中划出一部分 ${D_{val}}$ 作为验证集。${D}$ 另外的部分作为训练模型使用，${D_{val}}$ 独立开来，用来测试各个模型的好坏，最小化 ${E_{val}}$，从而选择最佳的gm∗。

## Validation

从训练集D中抽出一部分 ${K}$个数据作为验证集 ${D_{val}}$，${D_{val}}$ 对应的error记为 ${E_{val}}$ 。这样做的一个前提是保证 ${D_{val}}$ 独立同分布（${iid}$）于 ${P(x,y)}$，也就是说 ${D_{val}}$ 的选择是从 ${D}$ 中平均随机抽样得到的，这样能够把 ${E_{val}}$ 与 ${E_{out}}$ 联系起来。${D}$ 中去除 ${D_{val}}$ 后的数据就是供模型选择的训练数据 ${D_{train}}$，其大小为 ${N-k}$ 。从 ${D_{train}}$ 中选择最好的矩，记为 ${g^{-}(m)}$ 。

假如D共有1000个样本，那么可以选择其中900个 ${D_{train}}$，剩下的100个作为 ${D_{val}}$ 。使用 ${D_{train}}$ 训练模型，得到最佳的 ${g^{−}(m)}$，使用 ${g^{−}(m)}$ 对 ${D_{val}}$ 进行验证，得到如下Hoffding不等式：

假设有 ${M}$ 种模型 ${hypothesis\ set}$，${D_{val}}$ 的数量为 ${K}$，那么从每种模型m中得到一个在 ${D_{val}}$ 上表现最好的矩，再横向比较，从 ${M}$ 个矩中选择一个最好的m∗作为我们最终得到的模型。

现在由于数量为 ${N}$ 的总样本 ${D}$ 的一部分 ${K}$ 作为验证集，那么只有 ${N-k}$ 个样本可供训练。从 ${D_{train}}$中得到最好的 ${g^{−}(m)}$ ∗，而总样本 ${D}$ 对应的最好的矩为gm∗。根据之前的leraning curve很容易知道，训练样本越多，得到的模型越准确，其 ${hypothesis}$ 越接近 ${target\ function}$，即D的 ${E_{out}}$ 比 ${D_{train}}$ 的 ${E_{out}}$ 要小：

所以，我们通常的做法是通过 ${D_{val}}$ 来选择最好的矩 ${g^{−}(m)}$ ∗对应的模型m∗，再对整体样本集D使用该模型进行训练，最终得到最好的矩gm∗。

总结一下，使用验证集进行模型选择的整个过程为：先将D分成两个部分，一个是训练样本 ${D_{train}}$，一个是验证集 ${D_{val}}$ 。若有M个模型，那么分别对每个模型在 ${D_{train}}$ 上进行训练，得到矩 ${g^{−}(m)}$，再用 ${D_{val}}$ 对每个 ${g^{−}(m)}$ 进行验证，选择表现最好的矩 ${g^{−}(m)}$ ∗，则该矩对应的模型被选择。最后使用该模型对整个D进行训练，得到最终的gm∗。下图展示了整个模型选择的过程：

不等式关系满足：

下面我们举个例子来解释这种模型选择的方法的优越性，假设有两个模型：一个是5阶多项式HΦ5，一个是10阶多项式HΦ10。通过不使用验证集和使用验证集两种方法对模型选择结果进行比较，分析结果如下：

图中，横坐标表示验证集数量 ${K}$，纵坐标表示 ${E_{out}}$ 大小。黑色水平线表示没有验证集，完全使用 ${E_{in}}$ 进行判断基准，那么HΦ10更好一些，但是这种方法的 ${E_{out}}$ 比较大，而且与K无关。黑色虚线表示测试集非常接近实际数据，这是一种理想的情况，其 ${E_{out}}$ 很小，同样也与K无关，实际中很难得到这条虚线。红色曲线表示使用验证集，但是最终选取的矩是 ${g^{−}(m)}$ ∗，其趋势是随着K的增加，它对应的 ${E_{out}}$ 先减小再增大，当K大于一定值的时候，甚至会超过黑色水平线。蓝色曲线表示也使用验证集，最终选取的矩是gm∗，其趋势是随着K的增加，它对应的 ${E_{out}}$ 先缓慢减小再缓慢增大，且一直位于红色曲线和黑色直线之下。从此可见，蓝色曲线对应的方法最好，符合我们之前讨论的使用验证集进行模型选择效果最好。

这里提一点，当 ${K}$ 大于一定的值时，红色曲线会超过黑色直线。这是因为随着 ${K}$ 的增大，${D_{val}}$ 增大，但可供模型训练的 ${D_{train}}$ 在减小，那得到的 ${g^{−}(m)}$ ∗不具有很好的泛化能力，即对应的 ${E_{out}}$ 会增大，甚至当K增大到一定值时，比 ${E_{in}}$ 模型更差。

那么，如何设置验证集K值的大小呢？根据之前的分析：

当 ${K}$ 值很大时，${E_{val} \approx E_{out}}$，但是 ${g^{-}(m)}$ 与 ${g(m_}$ 相差很大；当K值很小是，${g^{-}(m) \approx gm}$，但是 ${E_{val}}$ 与 ${E_{out}}$ 可能相差很大。所以有个折中的办法，通常设置 ${k = \frac{N}{5}}$。值得一提的是，划分验证集，通常并不会增加整体时间复杂度，反而会减少，因为 ${D_{train}}$ 减少了。

## Leave-One-Out Cross Validation

假如考虑一个极端的例子，${k=1}$，也就是说验证集大小为1，即每次只用一组数据对 ${g(m)}$ 进行验证。这样做的优点是 ${g^{-}(m) \approx g(m)}$，但是 ${E_{val}}$ 与 ${E_{out}}$ 可能相差很大。为了避免 ${E_{val}}$ 与 ${E_{out}}$ 相差很大，每次从 ${D}$ 中取一组作为验证集，直到所有样本都作过验证集，共计算 ${N}$次，最后对验证误差求平均，得到 ${E_{loocv}(H,A)}$，这种方法称之为留一法交叉验证，表达式为：

这样求平均的目的是为了让 ${E_{loocv}(H,A)}$ 尽可能地接近 ${E_{out}(g)}$ 。

下面用一个例子图解留一法的过程：

如上图所示，要对二维平面上的三个点做拟合，上面三个图表示的是线性模型，下面三个图表示的是常数模型。对于两种模型，分别使用留一交叉验证法来计算 ${E_{loocv}}$，计算过程都是每次将一个点作为验证集，其他两个点作为训练集，最终将得到的验证误差求平均值，就得到了 ${E_{loocv}(linear)}$ 和 ${E_{loocv}(constant)}$，比较两个值的大小，取值小对应的模型即为最佳模型。

## 参考

1. [台湾大学林轩田机器学习基石课程学习笔记15 -- Validation](http://blog.csdn.net/red_stone1/article/details/72834968)